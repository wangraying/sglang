# Benchmark Results

## Machine Setup

- **GPUs**: NVIDIA A100-SXM4-40GB
- **CPU Cores**: 96
- **CPU Memory**: 1.5 TB
- **Software**: SGLang v0.3.5.post2, PyTorch 2.4.0
- **CUDA Version**: 12.4
- **Model**: Llama-3.1-8B-Instruct

## Datasets Overview

The datasets for benchmarking are as follows, each with varying sizes and characteristics.

| Dataset Name           | Number of sequences | Total Input Tokens | Total Output Tokens | Average System Prompt Length | Average Question Length |
|------------------------|---------------------|--------------------|---------------------|-------------------------|--------------------|
| Random-1000            | 3000                | 3,000,000          | 300,000             | -      | -   |
| Random-2000            | 3000                | 6,000,000          | 600,000             | -      | -   |
| Random-4000            | 3000                | 12,000,000         | 1,200,000           | -      | -   |
| Random                 | 3000                | 6,020,143          | 601,639             | -      | -   |
| ShareGPT               | 30000               | 6,664,740          | 5,940,589           | -      | -   |
| Generated-Shared-Prefix| 4096                | 10,948,645         | 1,048,576           | 2137.7 | 535.3

- **Random-*n***: A dataset of sequences consisting of randomly generated tokens, each sequence maintaining a fixed input length of *n*.
- **Random**: A dataset of sequences consisting of randomly generated tokens, each sequence has a random input length, the random range ratio is 0.5.
- **ShareGPT**: A dataset of sequences derived from [ShareGPT dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered).
- **Generated-Shared-Prefix**: A dataset of sequences organized in 128 groups, with sequences in each group starting with a common shared prefix.

## Metrics

The three primary metrics of interest are:
1. **Time to First Token (TTFT)**: measures the latency of generating the first output token.
2. **Inter-Token Latency (ITL)**: measures the interval between the generation of consecutive output tokens of a request.
3. **Output Throughput**: calculated by dividing the total number of tokens generated by the total duration of the request.

## Varying Scheduling Policies

### Experiment Settings

- The maximum number of tokens is set to 128K, and the request rate is fixed at 16.
- Vary the schedule policies among LPM (Longest-Prefix-Match), FCFS (First-Come-First-Serve), DFS-Weight, Random, and LOF (Longest-Output-First).
- Default values are used for all other parameters, such as the chunked prefill size is fixed to 8192 and mixed-running is not enabled.
- We adopt a data parallel (dp) size of 1 and a tensor parallel (tp) size of 1 throughout our experiments, following the default setting. We will not reiterate it in the subsequent sections.

### Performance

We begin by presenting the performance metrics gathered from the Random-2000 dataset, which could give us a clear numerical overview.
Then, we compare the performance metrics across all the six datasets using different schedule policies.

<p style="text-align: center;">
<em>Table: Performance Metrics collected on Random-2000 dataset.</em>
</p>

| Scheduling Policy | Request Throughput (req/s) | Output Token Throughput (tok/s) | Mean End-to-End Latency (ms) | Mean TTFT (ms) | TP95 TTFT (ms) | TP99 TTFT (ms) | Mean ITL (ms) | TP95 ITL (ms) | TP99 ITL (ms) |
|-------------------|----------------------------|---------------------------------|------------------------------|----------------|----------------|----------------|---------------|---------------|---------------|
| LPM               | 4.2                        | 844.0                           | 267208.8                     | 255247.3       | 487127.6       | 511899.5       | 60.3          | 30.7          | 188.1         |
| Random            | 4.2                        | 843.0                           | 267239.5                     | 256162.9       | 568143.0       | 633107.3       | 56.0          | 30.9          | 190.9         |
| FCFS              | 4.2                        | 842.9                           | 266770.4                     | 254763.2       | 487114.0       | 508571.2       | 60.4          | 30.8          | 174.8         |
| DFS-Weight        | 4.1                        | 818.4                           | 282485.2                     | 271800.6       | 566219.4       | 569575.0       | 53.8          | 31.6          | 245.3         |
| LOF               | 4.2                        | 844.0                           | 267246.1                     | 255257.9       | 487189.1       | 508590.9       | 60.4          | 30.7          | 183.0         |


**Output Throughput:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-vs-schedule-policy.png" alt="Output Throughput" style="width:50%; height:auto;"/>
</p>

**TTFT Latency:**

Figure (a) presents the P99 TTFT latency across various scheduling policies. For better visualization, normalize TTFT latency using the first value of each group. The normalized TTFT latency values are shown in Figure (b).

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-latency-vs-schedule-policy.png" alt="P99 TTFT for Different Schedule Policies" style="width:120%; height:auto;"><br>
      (a) P99 TTFT Latency for Different Schedule Policies
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-latency-vs-schedule-policy-normalized.png" alt=" P99 TTFT (Normalized) for Different Schedule Policies" style="width:90%; height:auto;"><br>
      (b) P99 TTFT Latency (Normalized) for Different Schedule Policies
    </td>
  </tr>
</table>

**ITL Latency:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-itl-latency-vs-schedule-policy.png" alt="P99 ITL Latency" style="width:50%; height:auto;"/>
</p>

### Observations

1. Random policy almost performs worse across all datasets, in terms of output throughput and TTFT. It is especially worse on
Generated-Shared-Prefix dataset, since it fails to exploit the characteristic of sharing common prefix among
consecutive requests in each sequence group, but it still needs to maintain the prefix structure in the radix tree. We added Figures (a), (b) and (c) below to support our observations.
2. LOF policy performs poorly in terms of TTFT on datasets with random length, i.e. Random dataset. This is expected since LOF
policy offers no guarantee on TTFT. Figure (d) has a better illustration of it.
3. FCFS and LPM policies outperform the others across all the datasets, in terms of output throughput and TTFT.
4. For ShareGPT dataset, the performance metrics of different schedule policies don't differ much.
5. By comparing the results of Random-*n* datasets, we could observe a trend of increasing TTFT as the number of input tokens increases for all schedule policies, this is easy to derive. We also observe a trend of decreasing ITL when using FCFS, LOF and LPM policies. See Figures (e) and (f) for more details.


<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/cache-hit-rate-vs-schedule-policy.png" alt="Cache Hit Rate for Different Schedule Policies"><br>
      (a) Cache Hit Rate for Different Schedule Policies
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/throughput-on-gen-vs-schedule-policy.png" alt="Throughput on Generated-Shared-Prefix dataset" style="width:120%; height:auto;"><br>
      (b) Throughput on Generated-Shared-Prefix dataset
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-latency-on-gen-vs-schedule-policy.png" alt="TTFT Latency on Generated-Shared-Prefix dataset" style="width:120%; height:auto;"><br>
      (c) TTFT Latency on Generated-Shared-Prefix dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-latency-on-random-vs-schedule-policy.png" alt="TTFT Latency on Random dataset"><br>
      (d) TTFT Latency on Random dataset
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-for-random-n-vs-schedule-policy.png" alt="TTFT Latency on Random-n dataset"><br>
      (e) TTFT Latency on Random-n dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/itl-for-random-n-vs-schedule-policy.png" alt="ITL Latency on Random-n dataset"><br>
      (f) ITL Latency on Random-n dataset
    </td>
  </tr>
</table>

## Enabling and Disabling Radix Cache

### Experiment Settings

- The maximum number of tokens is set to 128K, and the request rate is fixed at 16.
- When the radix cache is disabled, LPM and DFS-Weight policies are equivalent to FCFS policy. Therefore, we only compare FCFS, LOF and Random policies in this experiment.
- Default values are used for all other parameters, such as the chunked prefill size is fixed to 8192 and mixed-running is not enabled.

### Performance

**Output Throughput:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-w-wo-cache.png" alt="Output Throughput" style="width:50%; height:auto;"/>
</p>

**Latency:**
<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-w-wo-cache-normalized.png" alt="P99 TTFT Latency (Normalized) w./w.o Cache"><br>
      (a) P99 TTFT Latency (Normalized) w./w.o Cache
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-itl-w-wo-cache.png" alt="P99 ITL Latency w./w.o Cache"><br>
      (b) P99 ITL Latency w./w.o Cache
    </td>
  </tr>
</table>

### Observations

1. For Generated-Shared-Prefix dataset, enabling radix cache can significantly improve output throughput and decrease
TTFT latency. For other datasets, this performance improvement may not be as obvious. This is expected since enabling
radix cache allows for sharing common prefix among requests. For Generated-Shared-Prefix dataset, consecutive
requests within each sequence group share a long common system prompt, and this will greatly decrease computation
when radix cache is enabled.
2. Enabling radix cache could also lead to a higher ITL, we attribute it to the overhead of maintaining the prefix
structure in the radix tree.
3. Similar trends are observed when using chunked prefills with mixed-running enabled. Figures (a), (b) and (c) are included below to reinforce our observations.
4. We conclude that for scenarios that have a characteristic of sharing common prefix among requests, radix cache is
preferable to boost performance, otherwise, a simple key-value based chunk cache, i.e. the implentation when radix cache
is disabled, is sufficient to achieve good performance.

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-w-wo-cache-chunked-prefills-normalized.png" alt="Output Throughput (Normalized) w./w.o Cache with Chunked Prefills"><br>
      (a) Output Throughput (Normalized) w./w.o Cache with Chunked Prefills
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-w-wo-cache-chunked-prefills-normalized.png" alt="P99 TTFT Latency (Normalized) w./w.o Cache with Chunked Prefills"><br>
      (b) P99 TTFT Latency (Normalized) w./w.o Cache with Chunked Prefills
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-itl-w-wo-cache-chunked-prefills-normalized.png" alt="P99 ITL Latency (Normalized) w./w.o Cache with Chunked Prefills"><br>
      (c) P99 ITL Latency (Normalized) w./w.o Cache with Chunked Prefills
    </td>
  </tr>
</table>

## Varying Cache Sizes

### Experiment Settings

- The request rate is fixed at 16.
- Vary the maximum number of tokens (corresponding to the cache size) among 32K, 64K, and 128K.
- FCFS policy is used for scheduling.
- Default values are used for all other parameters, such as the chunked prefill size is fixed to 8192 and mixed-running is **not** enabled.

### Performance

**Output Throughput:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-output-throughput-vs-cache-size.png" alt="Output Throughput" style="width:50%; height:auto;"/>
</p>

**Latency:**
<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-p99-ttft-vs-cache-size-line-chart.png" alt="P99 TTFT Latency under Different Cache Sizes"><br>
      (a) P99 TTFT Latency under Different Cache Sizes
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-p99-itl-vs-cache-size-line-chart.png" alt="P99 ITL Latency under Different Cache Sizes"><br>
      (b) P99 ITL Latency under Different Cache Sizes
    </td>
  </tr>
</table>

### Observations

The size of radix cache is determined by the `max_total_tokens` parameter of the server. In our experiment, we observed higher throughput and reduced latencies as the cache size increased. This is expected, since `max_total_tokens` limits both the number of requests in a prefill batch and the number of running requests. 

For our experiments, we set a prefilled chunk size of 8192. Before reaching the `max_total_tokens` limit, the number of prefill tokens/requests can be constrained by the prefilled chunk size. This explains why we observed a similar number of requests in prefill batches for different cache sizes across all datasets. However, a larger cache size tends to result in fewer chunking requests, particularly when there is a high number of running requests. This also explains the reduced TTFT latency observed with larger cache sizes. We added Figure (a) and (c) to support our analysis.

For the Random-2000 and Random-4000 datasets, we noted that when the cache size is doubled, the number of running requests nearly doubles as well, as depicted in Figures (b) and (d). This could explain the observed increase in throughput and reduction in ITL latency, given the improved decoding efficiency associated with larger cache sizes.

For the ShareGPT dataset, the situation is slightly different. When the `max_total_tokens` exceeds 64K, both throughput and latency reach a plateau.
Upon reaching this point, we noticed a significant decrease in the number of queued requests, while the number of running requests remains almost the same, indicating the performance is bottlenecked by the request rate of the client. These findings are shown in Figures (e) and (f).

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-random-2000-new-seq-vs-cache-size.png" alt="Number of New Sequences in Prefilled Batches of Random-2000 Dataset" style="width:65%; height:auto;"><br>
      (a) Number of New Sequences in Prefilled Batches of Random-2000 Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-random-2000-running-req-vs-cache-size.png" alt="Number of Running Requests of Random-2000 Dataset"><br>
      (b) Number of Running Requests of Random-2000 Dataset
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-random-4000-new-seq-vs-cache-size.png" alt="Number of New Sequences in Prefilled Batches of Random-4000 Dataset" style="width:65%; height:auto;"><br>
      (c) Number of Sequences in Prefilled Batches of Random-4000 Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-random-4000-running-req-vs-cache-size.png" alt="Number of Running Requests of Random-4000 Dataset"><br>
      (d) Number of Running Requests of Random-4000 Dataset
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-sharegpt-queue-req-vs-cache-size.png" alt="Number of Queued Requests of ShareGPT Dataset" style="width:65%; height:auto;"><br>
      (e) Number of Queued Requests of ShareGPT Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/fcfs-sharegpt-running-req-vs-cache-size.png" alt="Number of Running Requests of ShareGPT Dataset"><br>
      (f) Number of Running Requests of ShareGPT Dataset
    </td>
  </tr>
</table>

## Performance with Chunked Prefills and Mixed-Running

### Experiment Settings

- The maximum number of tokens is set to 128K.
- Vary the prefill chunk sizes among 256, 512, 1024, 2048, 4096, 8192 and 16384.
- FCFS policy is used for scheduling.
- Default values are used for all other parameters, and radix cache is enabled by default.
- We reduce the request rate to 4 in this experiment in order to avoid excessive queuing and reduce the number of total requests to 500 correspondingly.

### Chunked Prefills w.o. Mixed-Running

**Throughput:**

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/input-throughput-vs-chunk-size-wo-mixed-running-rate4.png" alt="Input Throughput v.o. Mixed-Running"><br>
      (a) Input Throughput w.o. Mixed-Running
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-vs-chunk-size-wo-mixed-running-rate4.png" alt="Output Throughput v.o. Mixed-Running"><br>
      (b) Output Throughput w.o. Mixed-Running
    </td>
  </tr>
</table>

**Latency (Normalized):**

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-vs-chunk-size-wo-mixed-running-normalized-rate4.png" alt="P99 TTFT Latency w.o. Mixed-Running"><br>
      (c) P99 TTFT Latency w.o. Mixed-Running
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/itl-vs-chunk-size-wo-mixed-running-normalized-rate4.png" alt="P99 ITL Latency w.o. Mixed-Running"><br>
      (d) P99 ITL Latency w.o. Mixed-Running
    </td>
  </tr>
</table>

#### Observations

Typically for very small chunk sizes, there tend to be a high TTFT latency due to excessive chunking. This observation could be verified by comparing the number of prefill batches, as shown in Figures (a), (b), and (c) with the original total request count of 500.

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-1000-prefill-new-seq-wo-mixed-running.png" alt="Number of New Sequences in Prefilled Batches of Random-1000 Dataset"><br>
      (a) Number of New Sequences in Prefilled Batches of Random-1000 Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-2000-prefill-new-seq-wo-mixed-running.png" alt="Number of New Sequences in Prefilled Batches of Random-2000 Dataset"><br>
      (b) Number of New Sequences in Prefilled Batches of Random-2000 Dataset
    </td>
  </tr>
  <tr>
     <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-prefill-new-seq-wo-mixed-running.png" alt="Number of New Sequences in Prefilled Batches of Random-4000 Dataset"><br>
      (c) Number of New Sequences in Prefilled Batches of Random-4000 Dataset (Small Chunk Sizes)
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-prefill-new-seq-wo-mixed-running-2.png" alt="Number of New Sequences in Prefilled Batches of Random-4000 Dataset"><br>
      (d) Number of New Sequences in Prefilled Batches of Random-4000 Dataset (Large Chunk Sizes)
    </td>
  </tr>
</table>

The impact to TTFT differs across the datasets as the chunked prefill size increases. At an appropriate request rate, there is a chunk size threshold beyond which TTFT stabilizes. For the Random-1000 dataset, this threshold is at a chunk size of 1024, while for Random-2000, it is 2048. Figures (e) and (h) also demonstrate that the number of requests is relatively low at these threshold points.

For the Random-4000 dataset, the situation is different; we observe a continued decrease in TTFT as the chunked prefill size increases. This is expected since TTFT consists of queuing delay and prefill execution time. 
An increase in chunk size will increase the prefill batch size, which could improve prefill efficiency and consequently reduce the queuing time.
We observe that larger chunk sizes correspond to increased batch sizes, and the system exhibits excessive queuing when processing long sequences, as demonstrated in Figures (d) and (f).

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-1000-prefill-queue-req-wo-mixed-running.png" alt="Number of Queued Requests of Random-1000 Dataset"><br>
      (e) Number of Queued Requests of Random-1000 Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-prefill-queue-req-wo-mixed-running.png" alt="Number of Queued Requests of Random-4000 Dataset"><br>
      (f) Number of Queued Requests of Random-4000 Dataset
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-2000-prefill-queue-req-wo-mixed-running-1.png" alt="Number of Queued Requests of Random-2000 Dataset"><br>
      (g) Number of Queued Requests of Random-2000 Dataset (Small Chunk Sizes)
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-2000-prefill-queue-req-wo-mixed-running-2.png" alt="Number of Queued Requests of Random-2000 Dataset"><br>
      (h) Number of Queued Requests of Random-2000 Dataset (Large Chunk Sizes)
    </td> 
  </tr>
</table>

The impact to ITL is not obvious, and it varies with the characteristics of the datasets. The impact to throughput parallels our observations for TTFT; with the exception of the Random-1000 dataset, all chunk sizes yield comparable throughput at this request rate.

### Impact of Mixed-Running

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-perf-w-wo-mixed-running-rate4.png" alt="Performance on Random Dataset"><br>
      (a) Performance on Random Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-1000-perf-w-wo-mixed-running-rate4.png" alt="Performance on Random-1000 Dataset"><br>
      (b) Performance on Random-1000 Dataset
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-2000-perf-w-wo-mixed-running-rate4.png" alt="Performance on Random-2000 Dataset"><br>
      (c) Performance on Random-2000 Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-perf-w-wo-mixed-running-rate4.png" alt="Performance on Random-4000 Dataset"><br>
      (d) Performance on Random-4000 Dataset
    </td>
  </tr>
</table>

<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-ttft-w-wo-mixed-running-rate4.png" alt="P99 TTFT on Random-4000 Dataset" style="width:50%; height:auto;"/><br>
(f) P99 TTFT on Random-4000 Dataset
</p>

#### Observations

By comparing the performance with mixed-running both enabled and disabled, we observe an expected increase in output throughput, which is typically along with a decreased ITL latency across different datasets with the exception of the Random-4000 dataset. In that case, the ITL latency remains almost unchanged as the chunk size increases when mixed-running is disabled. But when mixed-running is enabled, the ITL latency increases all along with the chunk sizes.

For Random-4000 dataset, our observation indicate that enabling mixed-running with a chunk size of 256 significantly reduces the number of decode batches. This implies a considerable number of decode tokens are piggybacked with the prefill tokens, thereby enhancing the decode throughput and overall effiency. The reduction in TTFT and ITL can be largely attributed to decreased queuing delays, as illustrated in Figures (a), (b), and (c).

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-prefill-queue-req-w-wo-mixed-running.png" alt="Number of Queued Requests of Random-4000 Dataset when Prefilling"><br>
      (a) Number of Queued Requests of Random-4000 Dataset when Prefilling
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-decode-queue-req-w-wo-mixed-running-chunk256.png" alt="Number of Queued Requests of Random-4000 Dataset after Decoding"><br>
      (b) Number of Queued Requests of Random-4000 Dataset after Decoding (Chunk Size = 256)
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-decode-queue-req-w-wo-mixed-running.png" alt="Number of Queued Requests of Random-4000 Dataset after Decoding"><br>
      (c) Number of Queued Requests of Random-4000 Dataset after Decoding (Other Chunk Sizes)
    </td>
  </tr>
</table>

In fact, if we reduce the request rate to 1, the results show that, within the system's capacity, enabling mixed-chunk will improve ITL at the risk of hurting TTFT. More details can be found in Figures (d) and (e).

<table>
 <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-latency-w-wo-mixed-running-rate1.png" alt="P99 Latency on Random-4000 Dataset at Request Rate of 1"><br>
      (d) P99 Latency on Random-4000 Dataset at Request Rate of 1
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-prefill-queue-req-w-wo-mixed-running-rate1.png" alt="Number of Queued Requests of Random-4000 Dataset when Prefilling at Request Rate of 1"><br>
      (e) Number of Queued Requests of Random-4000 Dataset when Prefilling at Request Rate of 1
    </td>
  </tr>
</table>

### Mixed-Running with Varying Chunk Sizes

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-vs-chunk-size-w-mixed-running-normalized-rate4.png" alt="TTFT Latency (Normalized) w. Mixed Running with Different Chunk Sizes"><br>
      (a) TTFT Latency (Normalized) w. Mixed Running with Different Chunk Sizes
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/itl-vs-chunk-size-w-mixed-running-normalized-rate4.png" alt="ITL Latency  (Normalized) w. Mixed Running with Different Chunk Sizes"><br>
      (b) ITL Latency (Normalized) w. Mixed Running with Different Chunk Sizes
    </td>
    </tr>
</table>

#### Observations

When mixed-running is enabled, We observe an increased ITL latency as the chunked prefill size increases. This is because larger prefills will create *generation stalls*[[1]](https://www.usenix.org/system/files/osdi24-agrawal.pdf).
The impact to TTFT latency isn't quite straightforward; however, a trend similar to that observed with mixed-running disabled is noted.

The overall impact varies with the characteristics of the datasets, the system's capacity and the request rate. The optimal setting may need more sophisticated methods for exploration and exploitation.
To meet stringent TTFT requirements, too small chunk sizes should be avoided to prevent excessive chunking. Too large chunk sizes are also inadvisable as increasing batch size after the hardware is saturated will extend the latency[[2]](https://arxiv.org/pdf/2401.11181). Mixed-running is preferred in order to improve throughput and ITL, but it could potentially hurt TTFT.

However, when the request rate is too high, the first step should be to scale the system's capacity by adding more resources with carefully designed parallelism strategies. We leave this to future studies.