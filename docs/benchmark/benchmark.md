# Benchmark Results

## Machine Setup

- **GPUs**: NVIDIA A100-SXM4-40GB
- **CPU Cores**: 96
- **CPU Memory**: 1.5 TB
- **Software**: SGLang v0.3.5.post2, PyTorch 2.4.0
- **CUDA Version**: 12.4
- **Model**: Llama-3.1-8B-Instruct

## Datasets Overview

The datasets for benchmarking are as follows, each with varying sizes and characteristics.

| Dataset Name           | Number of sequences | Total Input Tokens | Total Output Tokens | Average System Prompt Length | Average Question Length |
|------------------------|---------------------|--------------------|---------------------|-------------------------|--------------------|
| Random-1000            | 3000                | 3,000,000          | 300,000             | -      | -   |
| Random-2000            | 3000                | 6,000,000          | 600,000             | -      | -   |
| Random-4000            | 3000                | 12,000,000         | 1,200,000           | -      | -   |
| Random                 | 3000                | 6,020,143          | 601,639             | -      | -   |
| ShareGPT               | 30000               | 6,664,740          | 5,940,589           | -      | -   |
| Generated-Shared-Prefix| 4096                | 10,948,645         | 1,048,576           | 2137.7 | 535.3

- **Random-*n***: A dataset of sequences consisting of randomly generated tokens, each sequence maintaining a fixed input length of *n*.
- **Random**: A dataset of sequences consisting of randomly generated tokens, each sequence has a random input length, the random range ratio is 0.5.
- **ShareGPT**: A dataset of sequences derived from [ShareGPT dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered).
- **Generated-Shared-Prefix**: A dataset of sequences organized in 128 groups, with sequences in each group starting with a common shared prefix.

## Metrics

The three primary metrics of interest are:
1. **Time to First Token (TTFT)**: measures the latency of generating the first output token.
2. **Inter-Token Latency (ITL)**: measures the interval between the generation of consecutive output tokens of a request.
3. **Output Throughput**: calculated by dividing the total number of tokens generated by the total duration of the request.

## Varying Scheduling Policies

### Experiment Settings

- The maximum number of tokens (corresponding to the cache size) is set to 128K, and the request rate is fixed at 16.
- Vary the schedule policies among LPM (Longest-Prefix-Match), FCFS (First-Come-First-Serve), DFS-Weight, Random, and LOF (Longest-Output-First).
- Default values are used for all other parameters, such as the chunked prefill size is fixed to 8192 and mixed-running is not enabled.
- Throughout our experiments, we studied a data parallel (dp) size of 1 and a tensor parallel (tp) size of 1, following the default setting. We will not reiterate it in the subsequent sections.

### Performance

We begin by presenting the performance metrics gathered from the Random-2000 dataset, which could give us a clear numerical overview.
Then, we compare the performance metrics across all the six datasets using different schedule policies.

<p style="text-align: center;">
<em>Table: Performance Metrics collected on Random-2000 dataset.</em>
</p>

| Scheduling Policy | Request Throughput (req/s) | Output Token Throughput (tok/s) | Mean End-to-End Latency (ms) | Mean TTFT (ms) | TP95 TTFT (ms) | TP99 TTFT (ms) | Mean ITL (ms) | TP95 ITL (ms) | TP99 ITL (ms) |
|-------------------|----------------------------|---------------------------------|------------------------------|----------------|----------------|----------------|---------------|---------------|---------------|
| LPM               | 4.2                        | 844.0                           | 267208.8                     | 255247.3       | 487127.6       | 511899.5       | 60.3          | 30.7          | 188.1         |
| Random            | 4.2                        | 843.0                           | 267239.5                     | 256162.9       | 568143.0       | 633107.3       | 56.0          | 30.9          | 190.9         |
| FCFS              | 4.2                        | 842.9                           | 266770.4                     | 254763.2       | 487114.0       | 508571.2       | 60.4          | 30.8          | 174.8         |
| DFS-Weight        | 4.1                        | 818.4                           | 282485.2                     | 271800.6       | 566219.4       | 569575.0       | 53.8          | 31.6          | 245.3         |
| LOF               | 4.2                        | 844.0                           | 267246.1                     | 255257.9       | 487189.1       | 508590.9       | 60.4          | 30.7          | 183.0         |


**Output Throughput:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-vs-schedule-policy.png" alt="Output Throughput" style="width:80%; height:auto;"/>
</p>

**TTFT Latency:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-latency-vs-schedule-policy.png" alt="P99 TTFT Latency" style="width:80%; height:auto;"/>
</p>

For better visualization, normalize TTFT latency using the first value of each group.

<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-latency-vs-schedule-policy-normalized.png" alt="P99 TTFT Latency" style="width:80%; height:auto;"/>
</p>

**ITL Latency:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-itl-latency-vs-schedule-policy.png" alt="P99 ITL Latency" style="width:80%; height:auto;"/>
</p>

**Cache Hit Rate:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/cache-hit-rate-vs-schedule-policy.png" alt="Cache Hit Rate" style="width:80%; height:auto;"/>
</p>

### Observations

1. Random policy almost performs worse across all datasets, in terms of output throughput and TTFT. It is especially worse on
Generated-Shared-Prefix dataset, since it fails to exploit the characteristic of sharing common prefix among
consecutive requests in each sequence group, but it still needs to maintain the prefix structure in the radix tree. We added Figures (a) and (b) below to support our observations.
2. LOF policy performs poorly in terms of TTFT on datasets with random length, i.e. Random dataset. This is expected since LOF
policy offers no guarantee on TTFT. Figure (c) has a better illustration of it.
3. FCFS and LPM policies outperform the others across all the datasets, in terms of output throughput and TTFT.
4. For ShareGPT dataset, the performance metrics of different schedule policies don't differ much.
5. By comparing the results of Random-*n* datasets, we could observe a trend of increasing TTFT as the number of input tokens increases for all schedule policies, this is easy to derive. We also observe a trend of decreasing ITL when using FCFS, LOF and LPM policies. See Figures (d) and (e) for more details.


<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/throughput-on-gen-vs-schedule-policy.png" alt="Throughput on Generated-Shared-Prefix dataset"><br>
      (a) Throughput on Generated-Shared-Prefix dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-latency-on-gen-vs-schedule-policy.png" alt="TTFT Latency on Generated-Shared-Prefix dataset"><br>
      (b) TTFT Latency on Generated-Shared-Prefix dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-latency-on-random-vs-schedule-policy.png" alt="TTFT Latency on Random dataset"><br>
      (c) TTFT Latency on Random dataset
    </td>
  </tr>
</table>
<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-for-random-n-vs-schedule-policy.png" alt="TTFT Latency on Random-n dataset"><br>
      (d) TTFT Latency on Random-n dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/itl-for-random-n-vs-schedule-policy.png" alt="ITL Latency on Random-n dataset"><br>
      (e) ITL Latency on Random-n dataset
    </td>
  </tr>
</table>

## Enabling and Disabling Radix Cache

### Experiment Settings

- The maximum number of tokens (corresponding to the cache size) is set to 128K, and the request rate is fixed at 16.
- When the radix cache is disabled, LPM and DFS-Weight policies are equivalent to FCFS policy. Therefore, we only compare FCFS, LOF and Random policies in this experiment.
- Default values are used for all other parameters, such as the chunked prefill size is fixed to 8192 and mixed-running is not enabled.

### Performance

**Output Throughput:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-w-wo-cache.png" alt="Output Throughput" style="width:80%; height:auto;"/>
</p>

**Latency:**
<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-w-wo-cache-normalized.png" alt="P99 TTFT Latency (Normalized) w./w.o Cache"><br>
      (a) P99 TTFT Latency (Normalized) w./w.o Cache
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-itl-w-wo-cache.png" alt="P99 ITL Latency w./w.o Cache"><br>
      (b) P99 ITL Latency w./w.o Cache
    </td>
  </tr>
</table>

### Observations

1. For Generated-Shared-Prefix dataset, enabling radix cache can significantly improve output throughput and decrease
TTFT latency. For other datasets, this performance improvement may not be as obvious. This is expected since enabling
radix cache allows for sharing common prefix among requests. For Generated-Shared-Prefix dataset, consecutive
requests within each sequence group share a long common system prompt, and this will greatly decrease computation
when radix cache is enabled.
2. Enabling radix cache could also lead to a higher ITL, we attribute it to the overhead of maintaining the prefix
structure in the radix tree.
3. Similar trends are observed when using chunked prefills with mixed-running enabled. Figures (a), (b) and (c) are included below to reinforce our observations.
4. We conclude that for scenarios that have a characteristic of sharing common prefix among requests, radix cache is
preferable to boost performance, otherwise, a simple key-value based chunk cache, i.e. the implentation when radix cache
is disabled, is sufficient to achieve good performance.

<table>
  <tr>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-w-wo-cache-chunked-prefills-normalized.png" alt="Output Throughput (Normalized) w./w.o Cache with Chunked Prefills"><br>
      (a) Output Throughput (Normalized) w./w.o Cache with Chunked Prefills
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-w-wo-cache-chunked-prefills-normalized.png" alt="P99 TTFT Latency (Normalized) w./w.o Cache with Chunked Prefills"><br>
      (b) P99 TTFT Latency (Normalized) w./w.o Cache with Chunked Prefills
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-itl-w-wo-cache-chunked-prefills-normalized.png" alt="P99 ITL Latency (Normalized) w./w.o Cache with Chunked Prefills"><br>
      (c) P99 ITL Latency (Normalized) w./w.o Cache with Chunked Prefills
    </td>
  </tr>
</table>

## Varying Cache Sizes

### Experiment Settings

- The request rate is fixed at 16.
- The chunked prefill size is set to 512, with mixed-running enabled.
- Vary the maximum number of tokens among 32K, 64K, and 128K.
- Default values are used for all other parameters, and LPM policy is used for scheduling.

### Performance

**Output Throughput:**
<p align="center">
<img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-vs-cache-size.png" alt="Output Throughput" style="width:80%; height:auto;"/>
</p>

**Latency:**
<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/mean-e2e-latency-vs-cache-size-normalized.png" alt="Mean End-to-End Latency under Different Cache Sizes"><br>
      (a) Mean End-to-End Latency (Normalized) under Different Cache Sizes
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-ttft-vs-cache-size-line-chart.png" alt="P99 TTFT Latency under Different Cache Sizes"><br>
      (b) P99 TTFT Latency under Different Cache Sizes
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/p99-itl-vs-cache-size-line-chart.png" alt="P99 ITL Latency under Different Cache Sizes"><br>
      (c) P99 ITL Latency under Different Cache Sizes
    </td>
  </tr>
</table>

### Observations

Since the size of the radix cache is determined by the `max_num_tokens` parameter of the server, increasing the cache size means increasing the batch size, which almost always leads to a higher throughput and reduced latencies. However, for the Random-1000 and ShareGPT datasets which have relatively shorter sequence lengths, these performance gains saturate after the cache size exceeds 64K.

## Performance with Chunked Prefills

### Experiment Settings

- The maximum number of tokens (corresponding to the cache size) is set to 128K, and the request rate is fixed at 16.
- Vary the prefill chunk sizes among 256, 512, 1024, 2048 and 8192.
- FCFS policy is used for scheduling.
- Default values are used for all other parameters, and radix cache is enabled by default.

### With Mixed-Running Disabled

**Throughput:**

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/input-throughput-vs-chunk-size-wo-mixed-running-line-chart.png" alt="Input Throughput v.o. Mixed-Running"><br>
      (a) Input Throughput w.o. Mixed-Running
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-vs-chunk-size-wo-mixed-running-line-chart.png" alt="Output Throughput v.o. Mixed-Running"><br>
      (b) Output Throughput w.o. Mixed-Running
    </td>
  </tr>
</table>

**Latency (Normalized):**

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-vs-chunk-size-wo-mixed-running-normalized.png" alt="P99 TTFT Latency w.o. Mixed-Running"><br>
      (c) P99 TTFT Latency w.o. Mixed-Running
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/itl-vs-chunk-size-wo-mixed-running-normalized.png" alt="P99 ITL Latency w.o. Mixed-Running"><br>
      (d) P99 ITL Latency w.o. Mixed-Running
    </td>
  </tr>
</table>

#### Observations

When mixed-running is disabled, we typically observe an increased input/output throughput and a decreased TTFT latency as the chunked prefill size increases. This is expected due to the higher prefill efficiency of larger prefills. The impact on ITL latency is not that obvious, as it varies with the characteristics of the datasets.

### With Mixed-Running Enabled

#### Impact of Enabling Mixed-Running

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/gen-perf-w-wo-mixed-running.png" alt="Performance on Generated-Shared-Prefix Dataset"><br>
      (a) Performance on Generated-Shared-Prefix Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-perf-w-wo-mixed-running.png" alt="Performance on Random Dataset"><br>
      (b) Performance on Random Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-1000-perf-w-wo-mixed-running.png" alt="Performance on Random-1000 Dataset"><br>
      (c) Performance on Random-1000 Dataset
    </td>
  </tr>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-2000-perf-w-wo-mixed-running.png" alt="Performance on Random-2000 Dataset"><br>
      (d) Performance on Random-2000 Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/random-4000-perf-w-wo-mixed-running.png" alt="Performance on Random-4000 Dataset"><br>
      (e) Performance on Random-4000 Dataset
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/sharegpt-perf-w-wo-mixed-running.png" alt="Performance on ShareGPT Dataset"><br>
      (f) Performance on ShareGPT Dataset
    </td>
  </tr>
</table>

#### Varying Chunk Sizes

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-vs-chunk-size-w-mixed-running-normalized.png" alt="TTFT Latency (Normalized) w. Mixed Running with Different Chunk Sizes"><br>
      (a) TTFT Latency  (Normalized) w. Mixed Running with Different Chunk Sizes
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/itl-vs-chunk-size-w-mixed-running-normalized.png" alt="ITL Latency  (Normalized) w. Mixed Running with Different Chunk Sizes"><br>
      (b) ITL Latency  (Normalized) w. Mixed Running with Different Chunk Sizes
    </td>
    </tr>
</table>

#### Varying Output Lengths

We also studied the performance across three datasets (including two newly created ones: Random-2000-400 and Random-2000-800), each with a consistent input length of 2000 and different output lengths of 200, 400, and 800, respectively. The corresponding output throughput and TTFT latency are detailed below:

<table>
  <tr>
  <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/output-throughput-vs-output-length-w-mixed-running.png" alt="Output Throughput on Datasets with Different Output Lengths"><br>
      (a) Output Throughput on Datasets with Different Output Lengths
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/ttft-vs-output-length-w-mixed-running.png" alt="TTFT Latency on Datasets with Different Output Lengths"><br>
      (b) TTFT Latency on Datasets with Different Output Lengths
    </td>
    <td align="center">
      <img src="https://raw.githubusercontent.com/wangraying/sglang/refs/heads/v0.3.5.post2-dev/docs/images/itl-vs-output-length-w-mixed-running.png" alt="ITL Latency on Datasets with Different Output Lengths"><br>
      (b) ITL Latency on Datasets with Different Output Lengths
    </td>
  </tr>
</table>

#### Observations

1. By comparing the performance with mixed-running both enabled and disabled, we observe an expected increase in output throughput, which is typically along with a decreased ITL latency across most datasets.
2. When mixed-running is enabled, We also observe an increased ITL latency as the chunked prefill size increases. This is because larger prefills will create *generation stalls*[[1]](https://www.usenix.org/system/files/osdi24-agrawal.pdf).
3. The impact to TTFT latency is not that obvious. For datasets with shorter contex lengths, such as Random-1000 and ShareGPT, a larger chunked prefill size is preferred.
4. The overall impact varies with the characteristics of the datasets and the parallelism strategies. The optimal setting may need more sophisticated methods for exploration and exploitation. But for tp size of 1, as it is the case in our experiments, setting chunked prefill size to 512 with mixed-running enabled seems to be a good point to start with.
